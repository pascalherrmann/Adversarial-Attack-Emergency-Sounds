{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 4: Very Deep Convolutional Neural Networks for Raw Waveforms\n",
    "* Reference: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/audio_classifier_tutorial.ipynb\n",
    "* Paper: https://arxiv.org/pdf/1610.00087.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%autosave 60\n",
    "\n",
    "### Imports ###\n",
    "import json\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "\n",
    "# Audio Player\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/nfs/students/summer-term-2020/project-4/data/dataset1/finalDataset/\"\n",
    "DATA_FILES = [\"training.json\", \"validation.json\", \"testing.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJSON(path):\n",
    "    with open(path ) as f:\n",
    "        d = json.load(f)\n",
    "        return d\n",
    "    \n",
    "dTrain, dVal = getJSON(DATA_PATH + DATA_FILES[0]), getJSON(DATA_PATH + DATA_FILES[1])\n",
    "\n",
    "N = len(dTrain)\n",
    "i_random = randint(0, N)\n",
    "print(\"found \" + str(N) + \" samples.\")\n",
    "print(dTrain[0])\n",
    "print(\"Sample \" + str(i_random) + \" | \" + str(dTrain[i_random][\"binary_class\"]) + \" | \" + str(dTrain[i_random][\"label_names\"]))\n",
    "ipd.Audio(dTrain[i_random][\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Dataset-Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmergencyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, jsonData):\n",
    "\n",
    "        self.paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for sample in jsonData:\n",
    "            self.paths.append(sample[\"path\"])\n",
    "            self.labels.append(1 if sample[\"binary_class\"] == \"positive\" else 0)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #print(self.paths[index])\n",
    "\n",
    "        path = self.paths[index]\n",
    "        if path == \"/nfs/students/summer-term-2020/project-4/data/dataset1/download/training_unbalanced/negative/wM5Qf5xXT8w.wav\":\n",
    "            path = \"/nfs/students/summer-term-2020/project-4/data/dataset1/download/training_unbalanced/negative/LrRe3G30fYM.wav\"\n",
    "        \n",
    "        sound = torchaudio.load(path, out = None, normalization = True)\n",
    "        \n",
    "        #load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)\n",
    "        soundData = sound[0][0] #self.mixer(sound[0])\n",
    "        soundData = torch.mean(sound[0], axis=0) #self.mixer(sound[0])\n",
    "        soundData = soundData.view(-1,1)\n",
    "        \n",
    "        #downsample the audio to ~8kHz\n",
    "        tempData = torch.zeros([160000,1]) #tempData accounts for audio clips that are too short\n",
    "        if soundData.numel() < 160000:\n",
    "            tempData[:soundData.numel()] = soundData[:]\n",
    "        else:\n",
    "            tempData[:] = soundData[:160000]\n",
    "        \n",
    "        soundData = tempData\n",
    "        soundFormatted = torch.zeros([32000,1])\n",
    "        soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n",
    "        soundFormatted = soundFormatted.permute(1, 0)\n",
    "        return soundFormatted, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def getPath(self, index):\n",
    "        return self.paths[index]\n",
    "\n",
    "    \n",
    "train_set = EmergencyDataset(dTrain)\n",
    "val_set = EmergencyDataset(dVal)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size = 128, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M5 (4 Conv Layers)\n",
    "class NetM5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        #print(x.shape)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim = 2)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# M11 (10 Conv Layers)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "\n",
    "        self.conv2a = nn.Conv1d(64, 64, 3)\n",
    "        self.bn2a = nn.BatchNorm1d(64)\n",
    "        self.conv2b = nn.Conv1d(64, 64, 3)\n",
    "        self.bn2b = nn.BatchNorm1d(64)        \n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv3a = nn.Conv1d(64, 128, 3)\n",
    "        self.bn3a = nn.BatchNorm1d(128)\n",
    "        self.conv3b = nn.Conv1d(128, 128, 3)\n",
    "        self.bn3b = nn.BatchNorm1d(128)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "\n",
    "        self.conv4a = nn.Conv1d(128, 256, 3)\n",
    "        self.bn4a = nn.BatchNorm1d(256)\n",
    "        self.conv4b = nn.Conv1d(256, 256, 3)\n",
    "        self.bn4b = nn.BatchNorm1d(256)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "\n",
    "        self.conv5 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.avgPool = nn.AvgPool1d(27) #input should be 512x30 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512, 2) # right now softmax for 2 classes; can be changed to sigmoid but doesnt matter\n",
    "        \n",
    "    def forward(self, out):\n",
    "        out = self.conv1(out)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.pool1(out)\n",
    "        \n",
    "        out = self.conv2a(out)\n",
    "        out = F.relu(self.bn2a(out))\n",
    "        out = self.conv2b(out)\n",
    "        out = F.relu(self.bn2b(out))\n",
    "        out = self.pool2(out)\n",
    "        \n",
    "        out = self.conv3a(out)\n",
    "        out = F.relu(self.bn3a(out))\n",
    "        out = self.conv3b(out)\n",
    "        out = F.relu(self.bn3b(out))\n",
    "        out = self.pool3(out)\n",
    "        \n",
    "        out = self.conv4a(out)\n",
    "        out = F.relu(self.bn4a(out))\n",
    "        out = self.conv4b(out)\n",
    "        out = F.relu(self.bn4b(out))\n",
    "        out = self.pool4(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = F.relu(self.bn5(out))\n",
    "        #print(out.shape)\n",
    "        out = self.avgPool(out)\n",
    "        out = out.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        out = self.fc1(out)\n",
    "        return F.log_softmax(out, dim = 2)\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)#0.01\n",
    "#optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss))\n",
    "    print('           TRAIN-ACC: {}/{} ({:.0f}%)'.format(correct, len(train_loader.dataset),100. * correct / len(train_loader.dataset)))\n",
    "    return (100. * correct / len(train_loader.dataset))\n",
    "            \n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "    print('\\nVAL-ACC: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    return (100. * correct / len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "'''\n",
    "for epoch in range(1, 25):\n",
    "    scheduler.step()\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [0.01]\n",
    "decays = [0.9]\n",
    "weight_decays = [0.0001, 0.0005]\n",
    "epochs = 50\n",
    "reports = [\"| Network       | Learning Rate | LR-Decay  | Weight-Decay | Epochs | acc TRAIN | acc VAL |\", \"|---------------|---------------|-----------|--------------|--------|-----------|---------|\"]\n",
    "\n",
    "best_model = None\n",
    "best_val_acc = 0\n",
    "for decay in decays:\n",
    "    for w_d in weight_decays:\n",
    "        lr in lrs:\n",
    "            model = Net()\n",
    "            model.to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = w_d)\n",
    "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay)\n",
    "            log_interval = 20\n",
    "            for epoch in range(1, epochs+1):\n",
    "                scheduler.step()\n",
    "                train_acc = train(model, epoch)\n",
    "                val_acc = test(model, epoch)\n",
    "                if epoch == epochs: \n",
    "                    print(\"LR=\"+str(lr) + \", Decay=\"+str(decay)+ \", Weight-Decay=\"+str(w_d)+\" - Val-Acc=\"+str(val_acc))\n",
    "                    reports.append(\"| {} | {}| {}  | {} | {} | {} | {} |\".format(\"M10\", lr, decay, w_d, epochs, train_acc, val_acc))\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model = model\n",
    "                    \n",
    "                    \n",
    "print(\"Best Validation Accuracy: {}%\".format(best_val_acc))\n",
    "print(\"Report:\")\n",
    "for r in reports: print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Network       | Learning Rate | LR-Decay  | Weight-Decay | Epochs | acc TRAIN | acc VAL |\n",
    "|---------------|---------------|-----------|--------------|--------|-----------|---------|\n",
    "| M10           | 0.01          | 0.9       | 0            | 25     | 82.75     | 83      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePerformanceMetrics(model):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in val_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tp += torch.sum(pred & target)\n",
    "            tn += torch.sum((pred == 0) & (target == 0))\n",
    "            fp += torch.sum(pred & (target == 0))\n",
    "            fn += torch.sum(target & (pred == 0))\n",
    "            \n",
    "    fp = fp.data.cpu().numpy()\n",
    "    tp = tp.data.cpu().numpy()\n",
    "    fn = fn.data.cpu().numpy()\n",
    "    tn = tn.data.cpu().numpy()\n",
    "    \n",
    "    acc = (tp + tn) / (fp + fn + tp + tn)\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f1 = 2*(prec*rec)/(prec+rec)\n",
    "    \n",
    "    print('Prec={:.2f}'.format(prec))\n",
    "    print('Rec={:.2f}'.format(rec))\n",
    "    print('F1={:.2f}'.format(f1))\n",
    "\n",
    "    print('\\nVAL-ACC: {}/{} ({:.0f}%)\\n'.format(correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    \n",
    "computePerformanceMetrics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for i in range(100):\n",
    "    randomIndex = randint(0, len(val_set)-1)\n",
    "    x, y = val_set.__getitem__(randomIndex)\n",
    "    x.unsqueeze_(0)\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    pred = output.max(2)[1]\n",
    "    if pred == y: correct += 1\n",
    "    else: \n",
    "        path = val_set.getPath(randomIndex)\n",
    "        print(path)\n",
    "        print(\"Predicted: \" + (\"EM\" if pred else \"NON-EM\") + \" - Label:\" + (\"EM\" if y else \"NON-EM\"))\n",
    "        ipd.display(ipd.Audio(path))\n",
    "        \n",
    "print(\"Correct classified: \" + str(correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
